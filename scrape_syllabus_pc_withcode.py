import requests
from bs4 import BeautifulSoup
import json
import time
import re

OUTPUT_FILENAME = "syllabus_2025_full_with_code.json"

# URLãƒªã‚¹ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
with open("syllabus_urls.json", encoding="utf-8") as f:
    syllabus_urls = json.load(f)

results = []

# ===========================
#   ç§‘ç›®ã‚³ãƒ¼ãƒ‰æŠ½å‡º
# ===========================
def extract_course_code(soup, detail_table, url):
    """
    ç§‘ç›®ã‚³ãƒ¼ãƒ‰ / æˆæ¥­ã‚³ãƒ¼ãƒ‰ã‚’å®‰å®šã—ã¦æŠ½å‡ºã™ã‚‹ã€‚
    å„ªå…ˆåº¦:
    (1) attribute ãƒ†ãƒ¼ãƒ–ãƒ«å†…ã®ã€Œæˆæ¥­ã‚³ãƒ¼ãƒ‰ / ç§‘ç›®ã‚³ãƒ¼ãƒ‰ã€
    (2) subjectTable01 å†…ã®ã€Œæˆæ¥­ã‚³ãƒ¼ãƒ‰ / ç§‘ç›®ã‚³ãƒ¼ãƒ‰ã€
    (3) URLã®æ•°å­— (fallback)
    """

    # --- (1) attribute ãƒ†ãƒ¼ãƒ–ãƒ«å„ªå…ˆ ---
    # ä¾‹: <td class="item"><span class="jp">æˆæ¥­ã‚³ãƒ¼ãƒ‰</span></td><td>H4020</td>
    for key, val in detail_table.items():
        if any(kw in key for kw in ["æˆæ¥­ã‚³ãƒ¼ãƒ‰", "ç§‘ç›®ã‚³ãƒ¼ãƒ‰", "ç§‘ç›®ç•ªå·", "ã‚³ãƒ¼ãƒ‰"]):
            return val.strip()

    # --- (2) subjectTable01 ---
    table = soup.find("table", class_="subjectTable01")
    if table:
        for tr in table.find_all("tr"):
            tds = tr.find_all("td")
            if len(tds) >= 2:
                key = tds[0].get_text(strip=True)
                val = tds[1].get_text(strip=True)
                if any(kw in key for kw in ["æˆæ¥­ã‚³ãƒ¼ãƒ‰", "ç§‘ç›®ã‚³ãƒ¼ãƒ‰", "ç§‘ç›®ç•ªå·", "ã‚³ãƒ¼ãƒ‰"]):
                    return val.strip()

    # --- (3) fallback: URL ã®æ•°å­—åˆ— ---
    m = re.search(r"(\d{6,})", url)
    if m:
        return m.group(1)

    return None


# ===========================
#   å„ãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡º
# ===========================
def extract_detail_table(soup):
    table_data = {}
    detail_table = soup.find("table", class_="attribute")
    if detail_table:
        for row in detail_table.find_all("tr"):
            tds = row.find_all("td")
            if len(tds) >= 2:
                key = tds[0].get_text(strip=True)
                val = tds[1].get_text(strip=True)
                table_data[key] = val
    return table_data



def extract_teaching_plan(soup):
    """
    æˆæ¥­è¨ˆç”»ãƒ†ãƒ¼ãƒ–ãƒ« (class="schedule") ã‹ã‚‰
    1å›ã”ã¨ã®æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹ã€‚

    åˆ—æ§‹æˆï¼ˆPCç‰ˆæƒ³å®šï¼‰:
      0: å› / No.
      1: methods of teaching
      2: ãƒ†ãƒ¼ãƒ / Theme
      3: å†…å®¹ / Contents
    """
    plan = []
    table = soup.find("table", class_="schedule")
    if not table:
        return plan

    rows = table.find_all("tr")
    for row in rows:
        cols = row.find_all("td")
        # ãƒ‡ãƒ¼ã‚¿è¡Œã ã‘ã‚’å¯¾è±¡ï¼ˆãƒ˜ãƒƒãƒ€è¡Œã«ã¯ td ãŒç„¡ã„ã“ã¨ãŒå¤šã„ï¼‰
        if len(cols) >= 4:
            week   = cols[0].get_text(strip=True)
            method = cols[1].get_text(strip=True)
            theme  = cols[2].get_text(strip=True)
            # <br> åŒºåˆ‡ã‚Šã‚’æ”¹è¡Œã§ã¤ãªã’ã‚‹
            content = cols[3].get_text("\n", strip=True)

            # ç©ºè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã‚‚ã„ã„
            if not week and not theme and not content:
                continue

            plan.append({
                "week": week,
                "method": method,
                "theme": theme,
                "content": content,
            })

    return plan



def extract_overview_from_subject_contents(soup):
    """
    <div class="subjectContents">
      <span class="jp">â€¦</span>
      <span class="en">â€¦</span>
    """
    overview_ja, overview_en = None, None
    div = soup.find("div", class_="subjectContents")
    if div:
        jp_span = div.find("span", class_="jp")
        if jp_span:
            jp_texts = [p.get_text(" ", strip=True) for p in jp_span.find_all("p")]
            overview_ja = "\n".join(jp_texts).strip()

        en_span = div.find("span", class_="en")
        if en_span:
            en_texts = [p.get_text(" ", strip=True) for p in en_span.find_all("p")]
            overview_en = "\n".join(en_texts).strip()

    return overview_ja, overview_en


# ===========================
#   æ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰â†’ key-value å¤‰æ›
# ===========================
def parse_overview_ja(raw_text):
    items = {}
    current_key = None
    current_val = []

    for line in raw_text.splitlines():
        header = re.match(r"ã€(.+?)ã€‘", line)
        if header:
            if current_key:
                items[current_key] = "\n".join(current_val).strip()
            current_key = header.group(1).strip()
            current_val = [line[header.end():].strip()]
        elif current_key:
            current_val.append(line.strip())

    if current_key:
        items[current_key] = "\n".join(current_val).strip()

    return items


# æ—¥æœ¬èª â†’ è‹±èªã‚­ãƒ¼å¤‰æ›ï¼ˆè¾æ›¸ï¼‰
jp_to_en_key = {
    "æˆæ¥­ã®æ¦‚è¦ã¨ç›®çš„ï¼ˆä½•ã‚’å­¦ã¶ã‹ï¼‰ / Outline and objectives": "outline",
    "åˆ°é”ç›®æ¨™ / Goal": "goal",
    "æˆæ¥­ã®é€²ã‚æ–¹ã¨æ–¹æ³• / Method(s)": "method",
    "æˆæ¥­è¨ˆç”» / Schedule": "schedule_note",
    "ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ•™ç§‘æ›¸ï¼‰ / Textbooks": "textbooks",
    "å‚è€ƒæ›¸ / References": "references",
    "æˆç¸¾è©•ä¾¡ã®æ–¹æ³•ã¨åŸºæº– / Grading criteria": "grading",
    "ãã®ä»–ã®é‡è¦äº‹é … / Others": "others"
    # å¿…è¦ã«å¿œã˜ã¦ã“ã“ã«è¿½åŠ 
}


# ===========================
#   ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
# ===========================
for idx, url in enumerate(syllabus_urls):
    url = url.replace("t_mode=sp", "t_mode=pc")
    print(f"ğŸ”„ {idx+1}/{len(syllabus_urls)}: {url}")
    # ãƒ†ã‚¹ãƒˆç”¨ï¼š1ä»¶ã ã‘å‹•ã‹ã™ã¨ãã¯ä¸‹ã®æ¡ä»¶åˆ†å²ã‚’èµ·å‹•ã—ã¦break
    #if idx > 0:
    #    break

    try:
        res = requests.get(url)
        res.raise_for_status()
        soup = BeautifulSoup(res.text, "html.parser")

        # ã‚¿ã‚¤ãƒˆãƒ«
        title_tag = soup.find("h1")
        title = title_tag.get_text(strip=True) if title_tag else None

        # æ•™å“¡åï¼ˆPCç‰ˆã¯æœ€åˆã® h2ï¼‰
        h2_tags = soup.find_all("h2")
        teacher = h2_tags[0].get_text(strip=True) if len(h2_tags) else None

        # è©³ç´°ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆattributeï¼‰
        detail_table = extract_detail_table(soup)

        # æ¦‚è¦
        overview_ja, overview_en = extract_overview_from_subject_contents(soup)

        structured_overview = {}
        if overview_ja:
            parsed = parse_overview_ja(overview_ja)
            structured_overview = {
                jp_to_en_key.get(k, k): v
                for k, v in parsed.items()
            }

        # æˆæ¥­è¨ˆç”»
        teaching_plan = extract_teaching_plan(soup)

        # ç§‘ç›®ã‚³ãƒ¼ãƒ‰ï¼ˆæˆæ¥­ã‚³ãƒ¼ãƒ‰ï¼‰
        course_code = extract_course_code(soup, detail_table, url)

        # å®Œå…¨ç‰ˆãƒ¬ã‚³ãƒ¼ãƒ‰
        results.append({
            "url": url,
            "course_code": course_code,
            "title": title,
            "teacher": teacher,
            "overview_structured": structured_overview,
            "overview_en": overview_en,
            "detail_table": detail_table,
            "teaching_plan": teaching_plan
        })

        time.sleep(1)

    except Exception as e:
        print(f"âŒ Error at {url}: {e}")
        continue


# ===========================
#   ä¿å­˜
# ===========================
with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"ğŸ‰ å®Œäº†: {OUTPUT_FILENAME} ã«å®Œå…¨ç‰ˆã‚’å‡ºåŠ›ã—ã¾ã—ãŸï¼")
