from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import json
import time

options = Options()
# options.add_argument("--headless=new")
driver = webdriver.Chrome(options=options)

base_url = "https://syllabus.hosei.ac.jp/web/web_search_show.php?search=show&nendo=2025&t_mode=sp&sort=admin26_80&gakubueng=AP&gakubu=%E7%90%86%E5%B7%A5%E5%AD%A6%E9%83%A8&template=t4&bunrui57=%E5%BF%9C%E7%94%A8%E6%83%85%E5%A0%B1%E5%B7%A5%E5%AD%A6%E7%A7%91_%E5%AD%A6%E7%A7%91%E5%B0%82%E9%96%80%E7%A7%91%E7%9B%AE&title_h2=%E5%AD%A6%E7%A7%91%E5%B0%82%E9%96%80%E7%A7%91%E7%9B%AE&title_h2_eng=Specialized%20%20subjects%20of%20faculty"
driver.get(base_url)

print("ğŸ”„ æˆæ¥­ä¸€è¦§ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸­...")

urls = []
base_domain = "https://syllabus.hosei.ac.jp"
visited_urls = set()

while True:
    time.sleep(2)  # JSèª­ã¿è¾¼ã¿å¾…æ©Ÿ

    soup = BeautifulSoup(driver.page_source, "html.parser")

    # âœ… æ—¥æœ¬èªã®æˆæ¥­ãƒªãƒ³ã‚¯ã‚’å«ã‚€ <li class="jp"> ã ã‘å–å¾—
    jp_list_items = soup.select("ul.normalList01 li.jp")

    count_this_page = 0
    for li in jp_list_items:
        a_tag = li.find("a", href=True)
        if a_tag and "preview.php" in a_tag["href"]:
            full_url = base_domain + "/web/" + a_tag["href"].lstrip("/")
            if full_url not in urls:
                urls.append(full_url)
                count_this_page += 1

    print(f"ğŸ“„ æˆæ¥­ãƒªãƒ³ã‚¯æ•°ï¼ˆã“ã®ãƒšãƒ¼ã‚¸ï¼‰: {count_this_page}")

    # âœ… æ¬¡ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
    next_button = soup.select_one("div.pagenav_area li.next a")
    if next_button:
        next_href = next_button.get("href")
        next_url = base_domain + next_href
        if next_url in visited_urls:
            print("ğŸ” ãƒ«ãƒ¼ãƒ—æ¤œå‡ºã§åœæ­¢")
            break
        visited_urls.add(next_url)
        print(f"â¡ï¸ æ¬¡ãƒšãƒ¼ã‚¸ã¸: {next_url}")
        driver.get(next_url)
    else:
        print("âœ… æ¬¡ãƒšãƒ¼ã‚¸ãªã—ã€‚çµ‚äº†ã€‚")
        break

driver.quit()

# âœ… ä¿å­˜
with open("syllabus_urls.json", "w", encoding="utf-8") as f:
    json.dump(urls, f, ensure_ascii=False, indent=2)

print(f"âœ… åˆè¨ˆ {len(urls)} ä»¶ã®JPãƒªãƒ³ã‚¯ã‚’ syllabus_urls.json ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

