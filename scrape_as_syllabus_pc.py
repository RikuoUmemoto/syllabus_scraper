import requests
from bs4 import BeautifulSoup
import json
import time
import re

OUTPUT_FILENAME = "syllabus_ap_2025_full.json"

# å…¨ä»¶å–å¾—ã«å¤‰æ›´
with open("syllabus_urls.json", encoding="utf-8") as f:
    syllabus_urls = json.load(f)

results = []

def extract_detail_table(soup):
    table_data = {}
    detail_table = soup.find("table", class_="attribute")
    if detail_table:
        for row in detail_table.find_all("tr"):
            th = row.find("th")
            td = row.find("td")
            if th and td:
                key = th.get_text(strip=True)
                value = td.get_text(strip=True)
                table_data[key] = value
    return table_data

def extract_teaching_plan(soup):
    plan = []
    schedule_table = soup.find("table", class_="schedule")
    if schedule_table:
        for row in schedule_table.find_all("tr"):
            cols = row.find_all("td")
            if len(cols) >= 3:
                plan.append({
                    "week": cols[0].get_text(strip=True),
                    "theme": cols[1].get_text(strip=True),
                    "content": cols[2].get_text(strip=True)
                })
    return plan

def extract_overview_from_subject_contents(soup):
    overview_ja, overview_en = None, None
    contents_div = soup.find("div", class_="subjectContents")
    if contents_div:
        jp_span = contents_div.find("span", class_="jp")
        if jp_span:
            jp_texts = [p.get_text(" ", strip=True) for p in jp_span.find_all("p") if p.get_text(strip=True)]
            overview_ja = "\n".join(jp_texts)
        en_span = contents_div.find("span", class_="en")
        if en_span:
            en_texts = [p.get_text(" ", strip=True) for p in en_span.find_all("p") if p.get_text(strip=True)]
            overview_en = "\n".join(en_texts)
    return overview_ja, overview_en

def parse_overview_ja(raw_text):
    items = {}
    current_key = None
    current_val = []

    lines = raw_text.splitlines()
    for line in lines:
        header_match = re.match(r"ã€(.+?)ã€‘", line)
        if header_match:
            if current_key:
                items[current_key] = "\n".join(current_val).strip()
            current_key = header_match.group(1).strip()
            current_val = [line[header_match.end():].strip()]
        elif current_key:
            current_val.append(line.strip())

    if current_key:
        items[current_key] = "\n".join(current_val).strip()

    return items

jp_to_en_key = {
    "æˆæ¥­ã®æ¦‚è¦ã¨ç›®çš„ï¼ˆä½•ã‚’å­¦ã¶ã‹ï¼‰ / Outline and objectives": "outline",
    "åˆ°é”ç›®æ¨™ / Goal": "goal",
    "ã“ã®æˆæ¥­ã‚’å±¥ä¿®ã™ã‚‹ã“ã¨ã§å­¦éƒ¨ç­‰ã®ãƒ‡ã‚£ãƒ—ãƒ­ãƒãƒãƒªã‚·ãƒ¼ã«ç¤ºã•ã‚ŒãŸã©ã®èƒ½åŠ›ã‚’ç¿’å¾—ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã‹ï¼ˆè©²å½“æˆæ¥­ç§‘ç›®ã¨å­¦ä½æˆä¸æ–¹é‡ã«æ˜ç¤ºã•ã‚ŒãŸå­¦ç¿’æˆæœã¨ã®é–¢é€£ï¼‰ / Which item of the diploma policy will be obtained by taking this class?": "diploma_policy",
    "æˆæ¥­ã§ä½¿ç”¨ã™ã‚‹è¨€èª / Default language used in class": "language",
    "æˆæ¥­ã®é€²ã‚æ–¹ã¨æ–¹æ³• / Method(s)": "method",
    "ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—ãƒ‡ã‚£ã‚¹ã‚«ãƒƒã‚·ãƒ§ãƒ³ã€ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆç­‰ï¼‰ã®å®Ÿæ–½ / Active learning in class (Group discussion, Debate.etc.)": "active_learning",
    "ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ¯ãƒ¼ã‚¯ï¼ˆå­¦å¤–ã§ã®å®Ÿç¿’ç­‰ï¼‰ã®å®Ÿæ–½ / Fieldwork in class": "fieldwork",
    "æˆæ¥­è¨ˆç”» / Schedule": "schedule_note",
    "æˆæ¥­æ™‚é–“å¤–ã®å­¦ç¿’ï¼ˆæº–å‚™å­¦ç¿’ãƒ»å¾©ç¿’ãƒ»å®¿é¡Œç­‰ï¼‰ / Work to be done outside of class (preparation, etc.)": "outside_work",
    "ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ•™ç§‘æ›¸ï¼‰ / Textbooks": "textbooks",
    "å‚è€ƒæ›¸ / References": "references",
    "æˆç¸¾è©•ä¾¡ã®æ–¹æ³•ã¨åŸºæº– / Grading criteria": "grading",
    "å­¦ç”Ÿã®æ„è¦‹ç­‰ã‹ã‚‰ã®æ°—ã¥ã / Changes following student comments": "student_feedback",
    "å­¦ç”ŸãŒæº–å‚™ã™ã¹ãæ©Ÿå™¨ä»– / Equipment student needs to prepare": "equipment",
    "ãã®ä»–ã®é‡è¦äº‹é … / Others": "others"
}

for idx, url in enumerate(syllabus_urls):
    url = url.replace("t_mode=sp", "t_mode=pc")
    print(f"ğŸ”„ {idx+1}/{len(syllabus_urls)}: {url}")
    try:
        res = requests.get(url)
        soup = BeautifulSoup(res.text, "html.parser")

        title_tag = soup.find("h1")
        title = title_tag.get_text(strip=True) if title_tag else None

        h2_tags = soup.find_all("h2")
        teacher = h2_tags[0].get_text(strip=True) if len(h2_tags) > 0 else None

        overview_ja, overview_en = extract_overview_from_subject_contents(soup)
        detail_table = extract_detail_table(soup)
        teaching_plan = extract_teaching_plan(soup)

        structured_overview = {}
        if overview_ja:
            parsed = parse_overview_ja(overview_ja)
            structured_overview = {jp_to_en_key.get(k, k): v for k, v in parsed.items()}

        results.append({
            "url": url,
            "title": title,
            "teacher": teacher,
            "overview_structured": structured_overview,
            "overview_en": overview_en,
            "detail_table": detail_table,
            "teaching_plan": teaching_plan
        })

        time.sleep(1)
    except Exception as e:
        print(f"âŒ Error at {url}: {e}")
        continue

with open(OUTPUT_FILENAME, "w", encoding="utf-8") as f:
    json.dump(results, f, ensure_ascii=False, indent=2)

print(f"âœ… å®Œäº†: {OUTPUT_FILENAME} ã«ä¿å­˜ã—ã¾ã—ãŸ")

